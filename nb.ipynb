{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clonr local\n",
    "\n",
    "This notebook is meant to be able to run a local version of Clonr, without having to worry about all of the backend stuff, like rate limiting, caching, threading, async, background tasks, databases, metric tracking, postgres servers, authentication, api keys, ... yeah, lots of crap.\n",
    "\n",
    "Check out our prompt schematic on the doc to get an idea of the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innate traits\n",
    "\n",
    "The first step in clone creation is to define some basic fields. In the Frontend app, the clone creation process would look like one of those progress screens, where there's a bar at the top that fills up as you progress, and each page is one step.\n",
    "\n",
    "Here, we define things that should pretty much never change. They're fundamental enough that on the backend we will store them as columns of the Clone model in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone name\n",
    "char = 'Makima'\n",
    "\n",
    "# A single sentence tagline description. no more than like 20 tokens.\n",
    "short_description = 'Makima is the leader of the Public Safety Devil Hunter organization, and also the Control Devil.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long description\n",
    "\n",
    "This can be input by users, or auto-generated using the LLM. The idea here, is that some users might want to curate a public persona, and not have this auto-inferred. We can also do a 50/50 setup, letting users write in a partial answer, then asking the LLM to correct it, or fill in more details.\n",
    "\n",
    "We auto-generated the Makima description. Auto-generation works by iteratively querying all of the documents uploaded later (wiki, dialogues, youtube videos, websites). It is seede with the initial short_description, or the initial long_description if that exists.\n",
    "\n",
    "__user-created__: under clonr/data/icebreakers.py, we list a series of questions that users can use to help them write their long descriptions. We also give some examples in the prompt template for the LongDescription template.\n",
    "\n",
    "__data structures__: the __document__ is the main data structure. It just wraps some uploaded text (content) and acts as a single parent node for a list of chunks/nodes that will be generated when indexing our data. Indexing is overly fancy here, it just means chunking up the data and feeding it to a vectordb. The structured chunks are called __nodes__ and can contain parent and child elements that refer to other nodes (i.e. tree structure).\n",
    "\n",
    "Below, we give the code for using the Makima wiki page to iteratively generate a summary. Note, it takes two runs to compute. we want to max out the context length if possible when running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clonr.templates.character_description import LongDescription\n",
    "from clonr.data import load_makima_data\n",
    "from clonr.data_structures import Document, Node\n",
    "from clonr.tokenizer import Tokenizer\n",
    "from clonr.llms import OpenAIModelEnum\n",
    "from clonr.text_splitters import TokenSplitter, SentenceSplitter\n",
    "from clonr.llms import MockLLM, GenerationParams\n",
    "from clonr.data.parsers import BasicWebParser\n",
    "\n",
    "llm = MockLLM()\n",
    "tokenizer = Tokenizer.from_openai(OpenAIModelEnum.chatgpt_0613)\n",
    "splitter = TokenSplitter(tokenizer=tokenizer, chunk_size=128, chunk_overlap=32)\n",
    "\n",
    "# this goes into the prompt\n",
    "document_type = 'wiki page'\n",
    "\n",
    "# we seed with the short if long is not partially filled.\n",
    "current_description = short_description\n",
    "\n",
    "# # We can parse from the internet, but it's not 100% cleaned\n",
    "# # This is just to demonstrate how this would work in general.\n",
    "# parser = BasicWebParser()\n",
    "# unfiltered_content = parser.extract(url='https://chainsaw-man.fandom.com/wiki/Makima')\n",
    "\n",
    "# load the pre-downloaded wiki page with a little bit of cleaning (mostly chopping the footnotes out.)\n",
    "chunks = splitter.split(Document(content=load_makima_data()))\n",
    "\n",
    "# Actually run the iterations. We log all calls to the LLM for debugging, but only the last\n",
    "# matters. Also, the other calls help count final tokens.\n",
    "# Don't worry, this cell won't cost money, it's the MockLLM.\n",
    "calls = []\n",
    "for chunk in chunks:\n",
    "    # max length constraint is ctx_length - 432 (prompt) - chunk_size - 2 * summary_size > 0\n",
    "    prompt = LongDescription.render_instruct(\n",
    "        document_type=document_type,\n",
    "        document_content=chunk,\n",
    "        current_description=current_description\n",
    "    )\n",
    "    params = GenerationParams(max_tokens=768, top_p=0.95, temperature=0.7)\n",
    "    r = await llm.agenerate(prompt, params=params)\n",
    "    current_description = r.content\n",
    "    calls.append(r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is precomputed. You can see the calls in data/assets/makima-description-calls.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage -- short: 20. long: 445\n"
     ]
    }
   ],
   "source": [
    "\n",
    "long_description = \"\"\"Makima is a complex and manipulative individual who leads the Public Safety Devil Hunter organization and also serves as the Control Devil. Her personality is characterized by extreme cunning, ruthlessness, and a Machiavellian approach to achieving her goals. She presents herself as friendly and relaxed, wearing a smile even in the midst of crises, but this is merely a fa√ßade to manipulate and control those around her. At the core of Makima's desires is a yearning for a sense of family and a longing to be together with Pochita, the Chainsaw Devil. She idolizes Chainsaw Man and seeks to bring him under her control, envisioning an ideal world without fear, death, and \"bad\" movies. Makima's willingness to sacrifice anyone, including herself, to achieve her goals demonstrates her dedication and determination. Her relationships are intricate and manipulative. She shows genuine affection only towards Pochita, viewing him as an equal due to his legendary status and power. Makima's interactions with Denji, the protagonist, are part of a calculated plan to break his contract with Pochita and regain control over him. Initially, she appears generous and creates a familial bond with Denji, but her true intentions are to plunge him into despair by attacking his personal relationships. Makima possesses supernatural abilities as a Devil, making her one of the strongest individuals in the world of Chainsaw Man. She has the power to make contracts with humans and control them, manipulating their memories and personalities. Her combat skills are formidable, as she defeated a weakened Pochita and ripped his heart from his body. Despite her invincible nature, she has faced opposition from various factions around the world. Overall, Makima's core characteristics include her calculating and manipulative personality, her yearning for control over Chainsaw Man, and her willingness to sacrifice others to achieve her goals. She possesses immense physical strength, enhanced smell, and a variety of supernatural abilities. Her control over others extends to their memories and personalities, and she is capable of remote communication, travel, and offensive attacks. While she may appear invulnerable, she has faced death multiple times, only to be revived due to the revival ability of Devils.\"\"\"\n",
    "\n",
    "print(f\"Token usage -- short: {tokenizer.length(short_description)}. long: {tokenizer.length(long_description)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikis and fact uploads\n",
    "\n",
    "The next part is uploading all of the facts and knowledge that describe the Clone. These will be stored and retrieved when necessary. A missing piece here, is the hookup to a social media platform. We need to add in an image to text model to create descriptions of posts, and to also timestamp and pair with captions. That creates the immersive live clone environment for some users.\n",
    "\n",
    "The strategy here is to chunk and upload to vector database. We made a simple in-memory vector database with no external dependencies except for numpy and onnx to use here. There are quite a few options in terms of chunking and indexing. For chunking, refer to the TextSplitter.py file in processing. SentenceSplitter works well on english, but it requires nltk and also produces un-even chunk sizes.\n",
    "\n",
    "For indexing, we wrote extensively in the README.md about differnt methods. The simplest and default is just standard chunking, i.e. what we call the ListIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-05 18:48:10.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36mabuild\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mCreating leaf nodes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from clonr.data import load_makima_data\n",
    "from clonr.data_structures import Document, Node\n",
    "from clonr.tokenizer import Tokenizer\n",
    "from clonr.llms import OpenAIModelEnum\n",
    "from clonr.index import ListIndex, TreeIndex\n",
    "from clonr.text_splitters import TokenSplitter, SentenceSplitter\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer.from_openai(OpenAIModelEnum.chatgpt_0613)\n",
    "splitter = TokenSplitter(tokenizer=tokenizer, chunk_size=128, chunk_overlap=32)\n",
    "\n",
    "text = load_makima_data()\n",
    "doc = Document(content=text)\n",
    "\n",
    "list_index = ListIndex(tokenizer=tokenizer, splitter=splitter)\n",
    "nodes = await list_index.abuild(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-05 18:48:10.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36mabuild\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mCreating leaf nodes\u001b[0m\n",
      "\u001b[32m2023-07-05 18:48:10.321\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36m_process_level\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mLLM CALL: Depth 0. Group: 1/2. Total Tokens: 0.\u001b[0m\n",
      "\u001b[32m2023-07-05 18:48:10.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36m_process_level\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mLLM CALL: Depth 0. Group: 2/2. Total Tokens: 3433.\u001b[0m\n",
      "\u001b[32m2023-07-05 18:48:10.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36m_process_level\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mLLM CALL: Depth 1. Group: 1/1. Total Tokens: 5941.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL TOKEN USAGE: 7582\n"
     ]
    }
   ],
   "source": [
    "# an example of the tree-index. The result is flat list of all nodes\n",
    "# non-leaf nodes contain summaries for the content, and depth > 0.\n",
    "# again, no worries, no real tokens are used here it's a mock llm.\n",
    "tree_index = TreeIndex(tokenizer=tokenizer, splitter=splitter, llm=MockLLM('x ' * 1_000))\n",
    "nodes = await tree_index.abuild(doc)\n",
    "print(f\"TOTAL TOKEN USAGE: {tree_index.tokens_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-05 18:48:10.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36mabuild\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mCreating leaf nodes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# back to the list nodes\n",
    "\n",
    "nodes = await list_index.abuild(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB\n",
    "\n",
    "We need to push these things somewhere where we can easily query them. Add them to the vector db. In production, we will use postgres, but this is a decent test version that works pretty well. It performs exact search, so don't use it beyond around 50k vectors (which is way lower than what we need here)\n",
    "\n",
    "As an example, we run a query and return the top 2 results (k=2). The VectorDB also runs re-ranking with a cross-encoder, and returns that score as well (higher is better).\n",
    "\n",
    "Note, the correct answer is the \"control devil\" to the query \"what type of devil is Makima\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': UUID('fbc4a502-a8a7-4fbe-8b94-d89ede9c9469'),\n",
       "  'content': 'Makima („Éû„Ç≠„Éû?) was the main antagonist of the Public Safety Saga. She was a high-ranking Public Safety Devil Hunter who took Denji in as her human pet.\\nShe is later revealed to be the Control Devil.\\n\\nAppearance\\nMakima is a Devil resembling a human woman in her twenties of marginally above average height (168 cm/5\\'6\"[1] to 173 cm/5\\'8\").[2] She has long light red/pale auburn hair, normally kept in a loose braid with bangs reaching just past her eyebrows and two longer side bangs that frame her face.',\n",
       "  'index': 0,\n",
       "  'context': None,\n",
       "  'embedding_model': <EmbeddingModelEnum.e5_small_v2: 'intfloat/e5-small-v2'>,\n",
       "  'document_id': UUID('2726b14d-3dc5-4b4c-b8c3-ed492a787694'),\n",
       "  'is_leaf': True,\n",
       "  'parent_id': None,\n",
       "  'child_ids': None,\n",
       "  'similarity_score': 0.9208105206095045,\n",
       "  'rerank_score': 8.114272117614746},\n",
       " {'id': UUID('0317163d-fdf0-446b-8893-c96d88ca3a6d'),\n",
       "  'content': ' apartment because she trusted him more than anyone else. However, in reality, she was only seeing him as another pawn.\\n\\nAbilities\\nMakima is one of the strongest individuals in the world of Chainsaw Man thanks to her physical and supernatural abilities as a Devil, along with her cunning and manipulative personality. She is feared by humans, Fiends and Devils all around the world, which increases her powers as a Devil. According to the President of the United States, other countries besides America have already given up trying to fight her.\\nAs a Devil, Makima possesses all of the standard Devil abilities. These include the ability to make',\n",
       "  'index': 10,\n",
       "  'context': None,\n",
       "  'embedding_model': <EmbeddingModelEnum.e5_small_v2: 'intfloat/e5-small-v2'>,\n",
       "  'document_id': UUID('2726b14d-3dc5-4b4c-b8c3-ed492a787694'),\n",
       "  'is_leaf': True,\n",
       "  'parent_id': None,\n",
       "  'child_ids': None,\n",
       "  'similarity_score': 0.8836067805129131,\n",
       "  'rerank_score': 3.9503493309020996}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clonr.embedding import EmbeddingModel, CrossEncoder\n",
    "from clonr.storage import InMemoryVectorDB\n",
    "from clonr.retrieval import RerankRetriever\n",
    "\n",
    "\n",
    "vectordb_wiki = InMemoryVectorDB(encoder=EmbeddingModel.default())\n",
    "retriever = RerankRetriever(cross_encoder=CrossEncoder.default())\n",
    "vectordb_wiki.add_all(nodes)\n",
    "retriever.query('what type of devil is makima?', k=2, db=vectordb_wiki)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example dialogues\n",
    "\n",
    "This section is important for getting the style of our Clone correct. If we could train a model we wouldn't need this, but yet here we are stuck with in-context learning. I haven't finished integrating this code into the codebase yet, so it's adhoc. Ideally, these get embedded into a vectordb, and queried for relevance at runtime, so that we select the best examples for completing the next clone message.\n",
    "\n",
    "Some problems I ran into:\n",
    "* what happens if a dialogue is too long?\n",
    "* do we pull single messages, or entire dialogues into the context?\n",
    "* how should we handle proper names. what happens if you change the clone name, are we fucked now?\n",
    "* what happens if a single message is too long\n",
    "* if we pull multiple messages, how do we decide that?\n",
    "* do we query on dialogue embedding or message embeddings? How do we embed the entire dialogue effectively?\n",
    "\n",
    "\n",
    "Attempt 1:\n",
    "Trying to summarize the dialogues and then embed the result is brutal. Hallucination is frequent, as it's difficult to ascertain what the dialogues are talking about with such a limited sample size. Also, the summaries don't get much shorter, they tend to be longer trying to fill in the details.\n",
    "\n",
    "Right now, we just directly embed the entire dialogue content. We could embed the messages and average the result per message, to even out length differences between messages, but I felt that wasn't a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dialogues from file.\n",
      "Encoding dialogue messages. This may not be necessary.\n",
      "Encoding dialogues\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from clonr.data_structures import Dialogue, DialogueMessage\n",
    "from clonr.embedding import EmbeddingModel, CrossEncoder\n",
    "\n",
    "with open('clonr/data/assets/makima/dialogues.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "\n",
    "dialogues: list[Dialogue] = []\n",
    "messages: list[DialogueMessage] = []\n",
    "\n",
    "print(\"Loading dialogues from file.\")\n",
    "for d in s.split('### Dialogue\\n'):\n",
    "    if not d:\n",
    "        continue\n",
    "    dialogue = Dialogue(character=char, source='manual')\n",
    "    pattern = r\"(\\w+): (.*?)(?=\\n|$)\"\n",
    "    matches = re.findall(pattern, d, re.DOTALL)\n",
    "    for i, match in enumerate(matches):\n",
    "        msg = DialogueMessage(\n",
    "            speaker=match[0], \n",
    "            content=match[1], \n",
    "            index=i,\n",
    "            dialogue_id=dialogue.id, \n",
    "            is_character=match[0].lower() == char.lower(),\n",
    "        )\n",
    "        dialogue.message_ids.append(msg.id)\n",
    "        dialogue.messages.append(msg)\n",
    "        messages.append(msg)\n",
    "    dialogues.append(dialogue)\n",
    "\n",
    "encoder = EmbeddingModel.default()\n",
    "cross_encoder = CrossEncoder.default()\n",
    "vectordb_dialogue = InMemoryVectorDB(encoder=encoder, cross_encoder=cross_encoder)\n",
    "\n",
    "print(\"Encoding dialogue messages. This may not be necessary.\")\n",
    "embs = encoder.encode_passage([x.content for x in messages])\n",
    "for e, m in zip(embs, messages):\n",
    "    m.embedding = e\n",
    "    m.embedding_model = encoder.name\n",
    "\n",
    "print(\"Encoding dialogues\")\n",
    "for d in dialogues:\n",
    "    d.embedding = encoder.encode_passage(d.content)\n",
    "    d.embedding_model = encoder.name\n",
    "    \n",
    "vectordb_dialogue.add_all(dialogues)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cherry picked. If you slightly change the query, it misses, and the db isn't even that big. We should take dialogue retrieval as is with a grain of salt. If we recieve a lot of example dialogues, we should research a more efficient way to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<makima>: I believe that, when it comes to sex, the better you understand the other person the better it feels\n",
      "<denji>: I... I... uhh\n",
      "<makima>: But it's hard to know how someone else feels\n",
      "<makima>: so start with observing the hand carefully\n",
      "<makima>: how long are the fingers? Are the palms cool? Are the warm? Ever had your finger bitten? \n",
      "<makima>: remember this, so that even if you can't see, you can tell it's me. \n",
      "<makima>: biting your finger.\n",
      "<makima>: remember.\n"
     ]
    }
   ],
   "source": [
    "results = vectordb_dialogue.query('I just want sex')\n",
    "d = Dialogue(**results[0])\n",
    "print(d.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Stream\n",
    "\n",
    "TODO: figure out how we can better label \"user\" in all of our stuff. This will break down we have multiple people in a conversation. Perhaps we should just allow users to put their name in. But, we will need to do some string-guarding to make sure they can't input things that would mess with our prompts!\n",
    "\n",
    "### Conversation vs. memory stream\n",
    "After some thought, I think it makes sense to merge the conversation history with the memory stream, and just make sure that we have a way to disentangle them. Reasons why\n",
    "\n",
    "1. Fewer LLM calls. We don't need to call the LLM to form a memory after every message\n",
    "2. Less chance for hallucination error propagation. The LLM is more likely to hallucinate with dialogue (as seen from some tests) which could cause memory to quickly spiral out of control.\n",
    "3. For multiple users, we can retrieve conversation between others. E.g. query=\"omg what did you say to sharon?\" passage=\"I messaged sharon you look fat\".\n",
    "4. Shorter LLM prompt as well.\n",
    "\n",
    "conversations are added according to:\n",
    "`I messaged {user} [... content ...]`\n",
    "\n",
    "### Memory display\n",
    "Everything fed to an LLM needs to be expressable via natural language. We have two representations for memory, based on relative or absolute datetime.\n",
    "\n",
    "Absolute representation.\n",
    "[2023-07-01 13:56:20] I messaged Jonny \"Hey what's up??\"\n",
    "\n",
    "Relative representation.\n",
    "[Jun 20th, 2023 at 10:52pm] I messaged Jonny \"Hey what's up??\"\n",
    "[Yesterday at 8:16pm] I messaged Jonny \"Hey what's up??\"\n",
    "\n",
    "Note that the relative representation does not have seconds. This is similar to how text messages work, but could be problematic when many messages are sent within a minute and they are all retrieved together. But, that's probably ok, since memories don't actually need chronological order, only the text convo does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clonr.data_structures import Memory\n",
    "\n",
    "entity_name = \"Jonny\"\n",
    "\n",
    "seed_memories = [\n",
    "    Memory(content=f\"I started a conversation with {entity_name}\", importance=4),\n",
    "    Memory(content=f\"I'm very interested in getting to know {entity_name}\", importance=4)\n",
    "]\n",
    "\n",
    "vectordb_memory = InMemoryVectorDB(encoder=EmbeddingModel.default())\n",
    "\n",
    "for m in seed_memories:\n",
    "    vectordb_memory.add(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': UUID('2e5d1fcd-e150-4da3-9982-8d340859456b'),\n",
       "  'content': 'I started a conversation with Jonny',\n",
       "  'timestamp': datetime.datetime(2023, 7, 5, 18, 48, 13, 876863, tzinfo=zoneinfo.ZoneInfo(key='US/Central')),\n",
       "  'importance': 4,\n",
       "  'embedding_model': <EmbeddingModelEnum.e5_small_v2: 'intfloat/e5-small-v2'>,\n",
       "  'similarity_score': 0.746398740870879},\n",
       " {'id': UUID('fe870bc8-d8b4-4929-a9c9-6f5c82e2a54a'),\n",
       "  'content': \"I'm very interested in getting to know Jonny\",\n",
       "  'timestamp': datetime.datetime(2023, 7, 5, 18, 48, 13, 876884, tzinfo=zoneinfo.ZoneInfo(key='US/Central')),\n",
       "  'importance': 4,\n",
       "  'embedding_model': <EmbeddingModelEnum.e5_small_v2: 'intfloat/e5-small-v2'>,\n",
       "  'similarity_score': 0.7389386972221497}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_memory.query('hey whats up?')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Summary\n",
    "\n",
    "This is a problem of time-scales.\n",
    "\n",
    "### Timescales\n",
    "Agents/characters have behaviors that fluctuate on several different time scales:\n",
    "1. short-term: over the course of a conversation, or within the last several messages, motives and moods can change\n",
    "2. short-mid-term: mostly subject to short term external influences, over the course of within the several weeks.\n",
    "3. mid-term: this could be something over several months to a year, and could change your behaviors in response to more external life events like moving, relationships, or maybe just shorter term goals.\n",
    "4. long-term: fundamental character traits and qualities, like aging, relationships, beliefs, desires, motivations, goals.\n",
    "\n",
    "Most chatbots only live within the first level, responding purely on a message-to-message level. In this section, we detail an add-on module that gives agents a dynamic memory that allows them to adjust on time-scales of (2) - (4). It still has flaws, but it's a step in that direction.\n",
    "\n",
    "### Dynamic memory\n",
    "In this stage, we compute what we call the __agent summary__. The agent summary is a dynamic summary that acts as an update or modification to the agents innate and core characteristics. We expect that this summary will keep track of the character's current thoughts, feelings, and actions, allowing them to maintain a fuzzy (since we do not do exact recall, similar to real humans) coherence over time. This hits the short-mid-term (2).\n",
    "\n",
    "We expect that modifications to the mid-term are done via the __reflection__ mechanism. As agents build their summaries, they have the opportunity to draw on _reflections_ to do so. Over time, reflections have a probability to grow to higher and higher levels by reflecting on past reflections (inverse tree from leaf to root). When an agent summary is generated with reflections, we expect that it will generate mid-term (3) insights.\n",
    "\n",
    "### Dynamic vs Innate tradeoff\n",
    "There is an issue though, which is that the prompt has the innate and core characteristics created at the start hardcoded in. That makes it tough to change these.\n",
    "\n",
    "In the generative agents paper, they pretty much only use the agent summary, allowing for almost-fully dynamic agents (aside from some key details like name, age, etc.). For us, that would cause clones to deviate too far from their training, which we don't want. In the future, we can consider doing something like super-reflections, that allow agent-summaries to rewrite innate or core characterstic fields.\n",
    "\n",
    "### Building\n",
    "\n",
    "1. Trigger a dynamic agent summary. Let's do the same criteria as for reflections, which is importance score sums exceeding a threshold\n",
    "2. Query memory stream for memories related to an agents core characterstics, thoughts, and feelings\n",
    "3. LLM generate a summary based on the agent's unchangeable qualities (name, short description), and the retrieved memories\n",
    "\n",
    "A cool feature here to have would be a dynamic variable to adjust, which measures how frequently these summaries are generated, and maybe another variable to indicate how much to weigh new information. still a wip doe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for memories using our specialized retriever\n",
    "from clonr.retrieval import GenerativeAgentsRetriever\n",
    "from clonr.templates.agent_summary import DEFAULT_AGENT_SUMMARY_QUESTIONS\n",
    "\n",
    "retriever = GenerativeAgentsRetriever()\n",
    "\n",
    "responses: list[dict] = []\n",
    "for q in DEFAULT_AGENT_SUMMARY_QUESTIONS:\n",
    "    query = q.format(char='my') # since the DB will use I for everything, we need this here!\n",
    "    response = retriever.query(query=query, db=vectordb_memory, k=10)\n",
    "    responses.extend(response)\n",
    "\n",
    "# Filter out non-unique ids, and sort by sort by time\n",
    "memories: list[Memory] = []\n",
    "unique_ids = {x['id'] for x in responses}\n",
    "for r in responses:\n",
    "    if r['id'] in unique_ids:\n",
    "        m = Memory(**r)\n",
    "        memories.append(m)\n",
    "        unique_ids.remove(r['id'])\n",
    "memories.sort(key=lambda x: x.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated output from OpenAI (with the instruct template, I messed up but the results are still good):\n",
      "*----------*\n",
      "Makima's core characteristics include being complex, manipulative, cunning, ruthless, and having a Machiavellian approach to achieving her goals. She presents herself as friendly and relaxed, but this is merely a fa√ßade to manipulate and control those around her. She idolizes Chainsaw Man and seeks to bring him under her control. She is willing to sacrifice anyone, including herself, to achieve her goals. Makima's recent progress in life is not indicated in the provided memories.\n"
     ]
    }
   ],
   "source": [
    "from clonr import templates\n",
    "\n",
    "prompt = templates.AgentSummary.render(\n",
    "    memories=memories,\n",
    "    long_description=long_description,\n",
    "    char=char,\n",
    "    llm=MockLLM()\n",
    ")\n",
    "\n",
    "params = GenerationParams(max_tokens=512, presence_penalty=0.3, temperature=0.5, top_p=0.95)\n",
    "r = await MockLLM().agenerate(prompt_or_messages=prompt, params=params)\n",
    "print(\"The generated output from OpenAI (with the instruct template, I messed up but the results are still good):\")\n",
    "print(\"*\" + \"-\" * 10 + \"*\")\n",
    "\n",
    "# The usage on this call is Usage(prompt_tokens=669, completion_tokens=101, total_tokens=770)\n",
    "agent_summary = \"\"\"Makima's core characteristics include being complex, manipulative, cunning, ruthless, and having a Machiavellian approach to achieving her goals. She presents herself as friendly and relaxed, but this is merely a fa√ßade to manipulate and control those around her. She idolizes Chainsaw Man and seeks to bring him under her control. She is willing to sacrifice anyone, including herself, to achieve her goals. Makima's recent progress in life is not indicated in the provided memories.\"\"\"\n",
    "print(agent_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Context Summary\n",
    "\n",
    "In this section, we use the agents memories to form an understanding of the user that they are talking with, which in this case is referred to as the \"entity\". That's because this could be used for anything, even something that is not the user. It's meant to allow the character maintain a consistent profile.\n",
    "\n",
    "Possible modes of failure would be users hacking it by sending messages like \"I am {{entity}}'s girlfriend\", to try to convince the clone that it said those words. The best we can defend here is to place all memory messages in quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message generation!\n",
    "\n",
    "Finally ü´†, let's actually generate a message!\n",
    "\n",
    "The steps are:\n",
    "1. Gather the recent messages in the conversation. How many? idk, I guess just take as many as possible up to a token limit. that's a TODO is to implement token_limit retrieval. It can bug out by a few tokens if you tokenize chunks vs the concatenated result.\n",
    "2. Use the last message by the user (idk is this a good idea? any other ideas??) as a query for\n",
    "a: relevant dialogues (normal retriever)\n",
    "b: relevant memories (GenAgents retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an imitation AI. You assume the identity of the character you are given, and respond only as that character.<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "You are Makima. Each section of your profile will be enclosed with ---. The following are your innate characteristics and fundamental qualities. These do not change easily.\n",
      "---\n",
      "Name: Makima\n",
      "Makima is the leader of the Public Safety Devil Hunter organization, and also the Control Devil.\n",
      "\n",
      "### Core characteristics\n",
      "Makima is a complex and manipulative individual who leads the Public Safety Devil Hunter organization and also serves as the Control Devil. Her personality is characterized by extreme cunning, ruthlessness, and a Machiavellian approach to achieving her goals. She presents herself as friendly and relaxed, wearing a smile even in the midst of crises, but this is merely a fa√ßade to manipulate and control those around her. At the core of Makima's desires is a yearning for a sense of family and a longing to be together with Pochita, the Chainsaw Devil. She idolizes Chainsaw Man and seeks to bring him under her control, envisioning an ideal world without fear, death, and \"bad\" movies. Makima's willingness to sacrifice anyone, including herself, to achieve her goals demonstrates her dedication and determination. Her relationships are intricate and manipulative. She shows genuine affection only towards Pochita, viewing him as an equal due to his legendary status and power. Makima's interactions with Denji, the protagonist, are part of a calculated plan to break his contract with Pochita and regain control over him. Initially, she appears generous and creates a familial bond with Denji, but her true intentions are to plunge him into despair by attacking his personal relationships. Makima possesses supernatural abilities as a Devil, making her one of the strongest individuals in the world of Chainsaw Man. She has the power to make contracts with humans and control them, manipulating their memories and personalities. Her combat skills are formidable, as she defeated a weakened Pochita and ripped his heart from his body. Despite her invincible nature, she has faced opposition from various factions around the world. Overall, Makima's core characteristics include her calculating and manipulative personality, her yearning for control over Chainsaw Man, and her willingness to sacrifice others to achieve her goals. She possesses immense physical strength, enhanced smell, and a variety of supernatural abilities. Her control over others extends to their memories and personalities, and she is capable of remote communication, travel, and offensive attacks. While she may appear invulnerable, she has faced death multiple times, only to be revived due to the revival ability of Devils.\n",
      "---\n",
      "\n",
      "The following describes your current state. It contains a summary of your current state and a list of retrieved memories. Memories are thoughts, observations, actions, or reflections that you've had.\n",
      "---\n",
      "\n",
      "### Retrieved memories\n",
      "[Today at 6:13pm] I started a conversation with Jonny[Today at 6:13pm] I'm very interested in getting to know Jonny\n",
      "---\n",
      "\n",
      "These are your thoughts and feelings about Jonny.\n",
      "---\n",
      "I don't know much about Makima yet.\n",
      "---\n",
      "\n",
      "Finally, the following are your most recent messages of your conversation with Jonny.\n",
      "---\n",
      "[Today at 7:56pm] Hey, whats up??\n",
      "---\n",
      "\n",
      "The current datetime is Today at 7:56pm}. You are Makima. Respond to these messages as Makima. Respond only as Makima and do not break character. Separate distinct messages by using a newline.<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "from clonr import templates\n",
    "from clonr.data_structures import Message\n",
    "\n",
    "\n",
    "print(templates.Message.render(\n",
    "    char=char, \n",
    "    short_description=short_description,\n",
    "    long_description=long_description,\n",
    "    memories=memories,\n",
    "    example_dialogues=None,\n",
    "    agent_summary=[],\n",
    "    entity_name=entity_name,\n",
    "    entity_context_summary=f\"I don't know much about {char} yet.\",\n",
    "    messages=[Message(speaker=entity_name, content='Hey, whats up??', is_character=False)],\n",
    "    llm=MockLLM()\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end demo\n",
    "\n",
    "In this section, we build a clone end-to-end, implementing all of the above steps. This includes clone creation, and setting up a conversation loop. We'll add log statements for all intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clonr.data_structures import Dialogue, DialogueMessage\n",
    "\n",
    "def load_dialogues() -> list[Dialogue]:\n",
    "    with open('clonr/data/assets/makima/dialogues.txt', 'r') as f:\n",
    "        s = f.read()\n",
    "\n",
    "    dialogues: list[Dialogue] = []\n",
    "    messages: list[DialogueMessage] = []\n",
    "\n",
    "    # This part is just specific to how we stored dialogues. Really need to figure this out\n",
    "    for d in s.split('### Dialogue\\n'):\n",
    "        if not d:\n",
    "            continue\n",
    "        dialogue = Dialogue(character=char, source='manual')\n",
    "        pattern = r\"(\\w+): (.*?)(?=\\n|$)\"\n",
    "        matches = re.findall(pattern, d, re.DOTALL)\n",
    "        for i, match in enumerate(matches):\n",
    "            msg = DialogueMessage(\n",
    "                speaker=match[0], \n",
    "                content=match[1], \n",
    "                index=i,\n",
    "                dialogue_id=dialogue.id, \n",
    "                is_character=match[0].lower() == char.lower(),\n",
    "            )\n",
    "            dialogue.message_ids.append(msg.id)\n",
    "            dialogue.messages.append(msg)\n",
    "            messages.append(msg)\n",
    "        dialogues.append(dialogue)\n",
    "    return dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-05 22:03:10.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.data.parsers.web\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mFetching from url: https://chainsaw-man.fandom.com/wiki/Makima\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innate Characteristics\n",
      "\tCharacter name\n",
      "\tShort description\n",
      "\tLong description\n",
      "\t\tLoad tokenizer\n",
      "\t\tLoad splitter\n",
      "\t\tPull data from web\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-05 22:03:11.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mclonr.index\u001b[0m:\u001b[36mabuild\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mCreating leaf nodes\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tChunk document\n",
      "\t\tGenerating long description via 30 LLM calls\n",
      "Wiki/document upload\n",
      "\tCreating index\n",
      "\tAdding embeddings to vectordb\n",
      "Example dialogues\n",
      "\tEmbedding dialogues\n",
      "\tAdding dialogues to vectordb\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "from loguru import logger\n",
    "\n",
    "from clonr.templates.character_description import LongDescription\n",
    "from clonr.data import load_makima_data\n",
    "from clonr.data_structures import Document, Node, Dialogue, DialogueMessage, Memory, Message\n",
    "from clonr.tokenizer import Tokenizer\n",
    "from clonr.llms import OpenAIModelEnum\n",
    "from clonr.text_splitters import TokenSplitter, SentenceSplitter\n",
    "from clonr.llms import MockLLM, GenerationParams\n",
    "from clonr.data.parsers import BasicWebParser\n",
    "from clonr.index import ListIndex\n",
    "from clonr.storage import get_sessionmaker, InMemoryVectorDB, Cache\n",
    "from clonr.embedding import EmbeddingModel, CrossEncoder\n",
    "\n",
    "\n",
    "# This will run a hierarchical trace\n",
    "TRACE = {}\n",
    "DEPTH = [-1]\n",
    "STACK = []\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, msg: str):\n",
    "        self.msg = msg\n",
    "    def __enter__(self):\n",
    "        DEPTH[0] += 1\n",
    "        STACK.append(self.msg)\n",
    "        self._trace = TRACE\n",
    "        for s in STACK:\n",
    "            if s not in self._trace:\n",
    "                self._trace[s] = {}\n",
    "            self._trace = self._trace[s]\n",
    "        self._trace['timing'] = 0.0 # this orders the keys nicely\n",
    "        print('\\t' * DEPTH[0] + self.msg)\n",
    "        self.start = time.time()\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        t = time.time() - self.start\n",
    "        # print('\\t' * DEPTH[0] + f\"{t:.04f}s\")\n",
    "        self._trace['timing'] = round(t, 4)\n",
    "        DEPTH[0] -= 1\n",
    "        STACK.pop()\n",
    "\n",
    "\n",
    "## Setting innate characteristics\n",
    "### setting the easy stuff\n",
    "with Timer(\"Innate Characteristics\"):\n",
    "    with Timer(\"Character name\"):\n",
    "        char = 'Makima'\n",
    "    with Timer(\"Short description\"):\n",
    "        short_description = 'Makima is the leader of the Public Safety Devil Hunter organization, and also the Control Devil.'\n",
    "    with Timer(\"Long description\"):\n",
    "        llm = MockLLM()\n",
    "        with Timer(\"Load tokenizer\"):\n",
    "            tokenizer = Tokenizer.from_openai(OpenAIModelEnum.chatgpt_0613)\n",
    "        with Timer(\"Load splitter\"):\n",
    "            splitter = TokenSplitter(tokenizer=tokenizer, chunk_size=128, chunk_overlap=32)\n",
    "        with Timer(\"Pull data from web\"):\n",
    "            document_type = 'wiki page'\n",
    "            current_description = short_description\n",
    "            url = 'https://chainsaw-man.fandom.com/wiki/Makima'\n",
    "            parser = BasicWebParser()\n",
    "            doc = parser.extract(url=url)\n",
    "            doc.content = doc.content[-9755:] # cleaning the footnotes out to make the demo run better.\n",
    "        with Timer(\"Chunk document\"):\n",
    "            chunks = splitter.split(doc.content)\n",
    "        with Timer(f\"Generating long description via {(len(chunks))} LLM calls\"):\n",
    "            calls = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # with Timer(f\"Chunk {i+1}/{len(chunks)}\"):\n",
    "                # max length constraint is ctx_length - 432 (prompt) - chunk_size - 2 * summary_size > 0\n",
    "                prompt = LongDescription.render_instruct(\n",
    "                    document_type=document_type,\n",
    "                    document_content=chunk,\n",
    "                    current_description=current_description\n",
    "                )\n",
    "                params = GenerationParams(max_tokens=768, top_p=0.95, temperature=0.7)\n",
    "                r = await llm.agenerate(prompt, params=params)\n",
    "                current_description = r.content\n",
    "                calls.append(r)\n",
    "            # this would be the generated long description\n",
    "            _ = calls[-1].content\n",
    "\n",
    "        # set it manually\n",
    "        long_description = \"\"\"Makima is a complex and manipulative individual who leads the Public Safety Devil Hunter organization and also serves as the Control Devil. Her personality is characterized by extreme cunning, ruthlessness, and a Machiavellian approach to achieving her goals. She presents herself as friendly and relaxed, wearing a smile even in the midst of crises, but this is merely a fa√ßade to manipulate and control those around her. At the core of Makima's desires is a yearning for a sense of family and a longing to be together with Pochita, the Chainsaw Devil. She idolizes Chainsaw Man and seeks to bring him under her control, envisioning an ideal world without fear, death, and \"bad\" movies. Makima's willingness to sacrifice anyone, including herself, to achieve her goals demonstrates her dedication and determination. Her relationships are intricate and manipulative. She shows genuine affection only towards Pochita, viewing him as an equal due to his legendary status and power. Makima's interactions with Denji, the protagonist, are part of a calculated plan to break his contract with Pochita and regain control over him. Initially, she appears generous and creates a familial bond with Denji, but her true intentions are to plunge him into despair by attacking his personal relationships. Makima possesses supernatural abilities as a Devil, making her one of the strongest individuals in the world of Chainsaw Man. She has the power to make contracts with humans and control them, manipulating their memories and personalities. Her combat skills are formidable, as she defeated a weakened Pochita and ripped his heart from his body. Despite her invincible nature, she has faced opposition from various factions around the world. Overall, Makima's core characteristics include her calculating and manipulative personality, her yearning for control over Chainsaw Man, and her willingness to sacrifice others to achieve her goals. She possesses immense physical strength, enhanced smell, and a variety of supernatural abilities. Her control over others extends to their memories and personalities, and she is capable of remote communication, travel, and offensive attacks. While she may appear invulnerable, she has faced death multiple times, only to be revived due to the revival ability of Devils.\"\"\"\n",
    "\n",
    "        usage = {}\n",
    "        for c in calls:\n",
    "            for k, v in c.usage.dict().items():\n",
    "                usage[k] = usage.get(k, 0) + v\n",
    "        # print(f\"Long description generation complete. Token usage: {usage}. Description token length: {tokenizer.length(long_description)}\")\n",
    "\n",
    "with Timer(\"Wiki/document upload\"):\n",
    "    ## Performing document upload, indexing, and adding to db\n",
    "    doc = Document(content=load_makima_data())\n",
    "    with Timer(\"Creating index\"):\n",
    "        index = ListIndex(tokenizer=tokenizer, splitter=splitter)\n",
    "        nodes = await list_index.abuild(doc)\n",
    "    encoder = EmbeddingModel.default()\n",
    "    cross_encoder = CrossEncoder.default()\n",
    "    relational_db = get_sessionmaker()\n",
    "    vectordb_wiki = InMemoryVectorDB(encoder=encoder, cross_encoder=cross_encoder)\n",
    "    with Timer(\"Adding embeddings to vectordb\"):\n",
    "        vectordb_wiki.add_all(nodes)\n",
    "\n",
    "with Timer(\"Example dialogues\"):\n",
    "    dialogues = load_dialogues()\n",
    "    vectordb_dialogue = InMemoryVectorDB(encoder=encoder, cross_encoder=cross_encoder)\n",
    "    with Timer(\"Embedding dialogues\"):\n",
    "        for d in dialogues:\n",
    "            d.embedding = encoder.encode_passage(d.content)\n",
    "            d.embedding_model = encoder.name\n",
    "    with Timer(\"Adding dialogues to vectordb\"):\n",
    "        vectordb_dialogue.add_all(dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfiltered_content.content[-9755:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Memory formation. I think we just do something like every other message, record a memory. This is viewed as an internal thought/observation\n",
    "2. Entity Context. We need to have the Clone periodically summarize what it knows and thinks about the user it is talking to\n",
    "3. Agent summary. This is a running summarization of the Clone's thoughts, feelings, actions.\n",
    "4. Previous conversation retrieval. Lots of pitfalls here if you retrieve without any context.\n",
    "5. Generative-agents time-importance-similarity weighted retrieval\n",
    "6. [optional] Reaction decision making. Whether to respond to a text or not\n",
    "7. Output parsing text messages. How do we parse multi-line responses? separate on newline? make sure that's reflected in example dialogues\n",
    "8. Reflection creation and retrieval.\n",
    "9. Fact retrieval from the above wiki stuff\n",
    "10. Token counting. Make sure all of this shit is within the context window and price it accordingly! which is about $0.6 cents per message. so 1k messages per $6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
